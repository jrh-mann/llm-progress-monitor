{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccf9856b",
   "metadata": {},
   "source": [
    "Just run all these cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9965467",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e08b669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "from einops import einsum\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561baa29",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec2e979",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a89ba94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight_tensor = torch.load('/workspace/llm-progress-monitor/qwen3_4b_weight_tensor.pt')\n",
    "weight_tensor = torch.load('qwen3_4b_weight_tensor.pt')\n",
    "model_name = 'Qwen/Qwen3-4B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfff48aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(model_name, device_map=device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f67b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ema_preds(log_preds, alpha=0.5):\n",
    "    given_alpha = alpha\n",
    "    preds_list = log_preds.exp().tolist()\n",
    "    \n",
    "    ema_preds = []\n",
    "    cur_ema = None\n",
    "    for i,pred in enumerate(preds_list):\n",
    "        # Use a smooth transition from 0.5 to given_alpha, reaching given_alpha at 200 tokens\n",
    "        alpha = given_alpha\n",
    "        if cur_ema is None:\n",
    "            cur_ema = pred\n",
    "        else:\n",
    "            cur_ema = alpha*(cur_ema-1) + (1-alpha)*pred #-1 because we have stepped one token\n",
    "        ema_preds.append(cur_ema)\n",
    "    return ema_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2f75a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_preds(activation, weight_tensor):\n",
    "    return einsum(\n",
    "        einsum(activation, weight_tensor, 'seq d_model, pca d_model -> seq pca').softmax(dim=1),\n",
    "        0.5+torch.arange(weight_tensor.shape[0]).to(device, dtype=torch.bfloat16),\n",
    "        'seq pca, pca -> seq'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4231cba7",
   "metadata": {},
   "source": [
    "# Vibe coded UIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5e2c84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (6.3.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from plotly) (2.6.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "447d7ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2e0bab6ca940569c8956caae7730b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Text Generation Progress</h3>'), Textarea(value='', description='Prompt:', layoâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create input text box for prompt\n",
    "prompt_input = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder='Enter your prompt here...',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='100%', height='80px')\n",
    ")\n",
    "\n",
    "# Create submit button\n",
    "submit_button = widgets.Button(\n",
    "    description='Generate Text',\n",
    "    button_style='success',\n",
    "    tooltip='Click to start text generation',\n",
    "    icon='play'\n",
    ")\n",
    "\n",
    "# Create progress bar widget\n",
    "progress_bar = widgets.FloatProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    description='Progress:',\n",
    "    bar_style='info',\n",
    "    style={'bar_color': '#20B2AA'},\n",
    "    orientation='horizontal'\n",
    ")\n",
    "\n",
    "# Create percentage label\n",
    "percentage_label = widgets.HTML(\n",
    "    value=\"<b>0.0%</b>\",\n",
    "    description='',\n",
    ")\n",
    "\n",
    "# Create horizontal box for progress bar and percentage\n",
    "progress_row = widgets.HBox([progress_bar, percentage_label])\n",
    "\n",
    "# Create text widget for token display\n",
    "token_display = widgets.HTML(\n",
    "    value=\"<b>Generated tokens will appear here...</b>\",\n",
    "    placeholder='',\n",
    "    description='',\n",
    ")\n",
    "\n",
    "# Create container for the widgets\n",
    "progress_container = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Text Generation Progress</h3>\"),\n",
    "    prompt_input,\n",
    "    submit_button,\n",
    "    progress_row,\n",
    "    token_display\n",
    "])\n",
    "\n",
    "# Display the widget\n",
    "display(progress_container)\n",
    "\n",
    "def on_submit_clicked(b):\n",
    "    # Reset progress\n",
    "    progress_bar.value = 0\n",
    "    percentage_label.value = \"<b>0.0%</b>\"\n",
    "    token_display.value = \"<b>Generating...</b>\"\n",
    "    \n",
    "    # Get prompt from input\n",
    "    prompt = prompt_input.value\n",
    "    # Apply chat template\n",
    "    prompt = model.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    cur_log_preds = []\n",
    "    n_tokens_generated = 0\n",
    "    generated_tokens = []\n",
    "\n",
    "    with model.generate(prompt, max_new_tokens=32768, do_sample=True) as tracer:\n",
    "        # Call .all() to apply intervention to each new token\n",
    "        with tracer.all():\n",
    "            activations = model.model.layers[15].output[0]\n",
    "            if len(activations.shape) == 1:\n",
    "                activations = activations.unsqueeze(0)\n",
    "            preds = get_log_preds(activations, weight_tensor).tolist()\n",
    "            if len(preds) > 1:\n",
    "                pass\n",
    "            else:\n",
    "                cur_log_preds+=preds\n",
    "                ema_preds = get_ema_preds(torch.tensor(cur_log_preds))\n",
    "                n_tokens_generated+=1\n",
    "                pred_tokens_remaining = ema_preds[-1]\n",
    "                pred_percent_through = n_tokens_generated/(n_tokens_generated + pred_tokens_remaining)\n",
    "                \n",
    "                token = model.lm_head.output.argmax(dim=-1).tolist()\n",
    "                token_str = model.tokenizer.decode(token[0][0], skip_special_tokens=True)\n",
    "                generated_tokens.append(token_str)\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.value = pred_percent_through * 100\n",
    "                \n",
    "                # Update percentage label\n",
    "                percentage_label.value = f\"<b>{pred_percent_through*100:.1f}%</b>\"\n",
    "                \n",
    "                # Update token display with all generated tokens\n",
    "                tokens_html = \" \".join([f\"<span style='background-color: #e6f3ff; padding: 2px 4px; margin: 1px; border-radius: 3px;'>{token}</span>\" for token in generated_tokens])\n",
    "                token_display.value = f\"<b>Generated tokens:</b><br>{tokens_html}<br><br><b>Latest:</b> '{token_str}' | <b>Predicted:</b> {pred_percent_through*100:.1f}% through\"\n",
    "\n",
    "# Connect button click to function\n",
    "submit_button.on_click(on_submit_clicked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "167849a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bda30104d6479b91e0e9b73fcd117d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Text Generation Progress</h3>'), Textarea(value='', description='Prompt:', layoâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import ipywidgets as widgets\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "# Create input text box for prompt\n",
    "prompt_input = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder='Enter your prompt here...',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='100%', height='80px')\n",
    ")\n",
    "\n",
    "# Create input text box for EMA factor\n",
    "ema_input = widgets.FloatText(\n",
    "    value=0.9,\n",
    "    description='EMA Factor:',\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    tooltip='Exponential Moving Average factor (0.0 to 1.0)',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "# Create submit button\n",
    "submit_button = widgets.Button(\n",
    "    description='Generate Text',\n",
    "    button_style='success',\n",
    "    tooltip='Click to start text generation',\n",
    "    icon='play'\n",
    ")\n",
    "\n",
    "# Create progress bar widget\n",
    "progress_bar = widgets.FloatProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    description='Progress:',\n",
    "    bar_style='info',\n",
    "    style={'bar_color': '#20B2AA'},\n",
    "    orientation='horizontal'\n",
    ")\n",
    "\n",
    "# Create percentage label\n",
    "percentage_label = widgets.HTML(\n",
    "    value=\"<b>0.0%</b>\",\n",
    "    description='',\n",
    ")\n",
    "\n",
    "# Create horizontal box for progress bar and percentage\n",
    "progress_row = widgets.HBox([progress_bar, percentage_label])\n",
    "\n",
    "# Create text widget for token display\n",
    "token_display = widgets.HTML(\n",
    "    value=\"<b>Generated tokens will appear here...</b>\",\n",
    "    placeholder='',\n",
    "    description='',\n",
    ")\n",
    "\n",
    "# Create graph widget for prediction history\n",
    "graph_widget = go.FigureWidget()\n",
    "\n",
    "# Create container for the widgets\n",
    "progress_container = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Text Generation Progress</h3>\"),\n",
    "    prompt_input,\n",
    "    ema_input,\n",
    "    submit_button,\n",
    "    progress_row,\n",
    "    token_display,\n",
    "    widgets.HTML(\"<h4>Prediction History</h4>\"),\n",
    "    graph_widget\n",
    "])\n",
    "\n",
    "# Display the widget\n",
    "display(progress_container)\n",
    "\n",
    "# Global variables to store generation data\n",
    "raw_log_preds = []\n",
    "stored_generated_tokens = []\n",
    "stored_n_tokens = 0\n",
    "\n",
    "def get_color_for_change(percent_change, is_increase):\n",
    "    \"\"\"\n",
    "    Generate a color based on the magnitude of percent change.\n",
    "    Returns RGB color with gradient intensity based on change magnitude.\n",
    "    \"\"\"\n",
    "    # Clamp percent_change to reasonable range (0-50%)\n",
    "    clamped_change = min(percent_change, 50)\n",
    "    \n",
    "    # Normalize to 0-1 scale\n",
    "    intensity = clamped_change / 50\n",
    "    \n",
    "    if is_increase:\n",
    "        # Red gradient: from light blue (#e6f3ff) to deep red (#ff0000)\n",
    "        # Start: (230, 243, 255), End: (255, 0, 0)\n",
    "        r = int(230 + (255 - 230) * intensity)\n",
    "        g = int(243 - 243 * intensity)\n",
    "        b = int(255 - 255 * intensity)\n",
    "    else:\n",
    "        # Green gradient: from light blue (#e6f3ff) to deep green (#00cc00)\n",
    "        # Start: (230, 243, 255), End: (0, 204, 0)\n",
    "        r = int(230 - 230 * intensity)\n",
    "        g = int(243 - 39 * intensity)  # Goes to 204\n",
    "        b = int(255 - 255 * intensity)\n",
    "    \n",
    "    return f\"#{r:02x}{g:02x}{b:02x}\"\n",
    "\n",
    "def update_graph_with_ema(ema_factor):\n",
    "    \"\"\"Update the graph and displays with a new EMA factor.\"\"\"\n",
    "    global raw_log_preds, stored_generated_tokens, stored_n_tokens\n",
    "    \n",
    "    if len(raw_log_preds) == 0:\n",
    "        return\n",
    "    \n",
    "    # Recalculate predictions with new EMA\n",
    "    ema_preds = get_ema_preds(torch.tensor(raw_log_preds), alpha=ema_factor)\n",
    "    \n",
    "    prediction_history = []\n",
    "    token_counts = []\n",
    "    \n",
    "    for i, ema_pred in enumerate(ema_preds):\n",
    "        n_tokens_generated = i + 1\n",
    "        pred_tokens_remaining = ema_pred\n",
    "        predicted_total_tokens = n_tokens_generated + pred_tokens_remaining\n",
    "        prediction_history.append(predicted_total_tokens)\n",
    "        token_counts.append(n_tokens_generated)\n",
    "    \n",
    "    # Update the graph\n",
    "    if len(prediction_history) > 1:\n",
    "        # Create hover text with context tokens\n",
    "        hover_texts = []\n",
    "        for i in range(len(stored_generated_tokens)):\n",
    "            # Get 5 tokens before and after (if available)\n",
    "            start_idx = max(0, i - 5)\n",
    "            end_idx = min(len(stored_generated_tokens), i + 6)\n",
    "            \n",
    "            context_tokens = []\n",
    "            for j in range(start_idx, end_idx):\n",
    "                token_clean = stored_generated_tokens[j].replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>').replace('&quot;', '\"').replace('&#x27;', \"'\")\n",
    "                if j == i:\n",
    "                    context_tokens.append(f\"<b>{token_clean}</b>\")\n",
    "                else:\n",
    "                    context_tokens.append(token_clean)\n",
    "            \n",
    "            context_str = \" \".join(context_tokens)\n",
    "            hover_text = f\"Token {i+1}: {context_str}<br>Predicted Total: {prediction_history[i]:.0f}\"\n",
    "            hover_texts.append(hover_text)\n",
    "        \n",
    "        # Update graph with new data\n",
    "        graph_widget.data = []\n",
    "        graph_widget.add_trace(go.Scatter(\n",
    "            x=token_counts,\n",
    "            y=prediction_history,\n",
    "            mode='lines+markers',\n",
    "            name='Predicted Total Tokens',\n",
    "            line=dict(color='blue', width=2),\n",
    "            marker=dict(size=6),\n",
    "            hovertemplate='%{customdata}<extra></extra>',\n",
    "            customdata=hover_texts\n",
    "        ))\n",
    "        \n",
    "        # Add actual final point\n",
    "        graph_widget.add_trace(go.Scatter(\n",
    "            x=[stored_n_tokens],\n",
    "            y=[stored_n_tokens],\n",
    "            mode='markers',\n",
    "            name='Actual Final',\n",
    "            marker=dict(size=10, color='green', symbol='star'),\n",
    "            hovertemplate=f'Actual completion: {stored_n_tokens} tokens<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        # Update layout\n",
    "        graph_widget.update_layout(\n",
    "            title=f'Token Prediction vs Reality (EMA={ema_factor:.2f})',\n",
    "            xaxis_title='Token Number',\n",
    "            yaxis_title='Predicted Total Tokens',\n",
    "            height=400,\n",
    "            showlegend=True,\n",
    "            hovermode='closest'\n",
    "        )\n",
    "        \n",
    "        # Update token display with final statistics - preserve the colored tokens\n",
    "        final_pred = prediction_history[-1]\n",
    "        accuracy = (stored_n_tokens/final_pred)*100\n",
    "        \n",
    "        # Rebuild the colored token display with gradient colors\n",
    "        highlighted_tokens = []\n",
    "        for i, token in enumerate(stored_generated_tokens):\n",
    "            # Calculate percentage change if we have history\n",
    "            if i > 0 and i < len(prediction_history):\n",
    "                change = prediction_history[i] - prediction_history[i-1]\n",
    "                percent_change = abs(change / prediction_history[i-1]) * 100 if prediction_history[i-1] != 0 else 0\n",
    "                highlight_color = get_color_for_change(percent_change, change > 0)\n",
    "            else:\n",
    "                highlight_color = \"#e6f3ff\"  # Default light blue for first token\n",
    "            \n",
    "            highlighted_tokens.append(f\"<span style='background-color: {highlight_color}; padding: 2px 4px; margin: 1px; border-radius: 3px;'>{token}</span>\")\n",
    "        \n",
    "        tokens_html = \" \".join(highlighted_tokens)\n",
    "        token_display.value = f\"<b>Generation complete!</b><br><b>Total tokens:</b> {stored_n_tokens}<br><b>Final prediction:</b> {final_pred:.0f} tokens<br><b>Accuracy:</b> {accuracy:.1f}%<br><b>Current EMA:</b> {ema_factor:.2f}<br><br><b>Generated tokens:</b><br>{tokens_html}<br><br><small><b>Color coding:</b> Gradient from <span style='background-color: #e6f3ff; padding: 2px;'>neutral</span> to <span style='background-color: #ff8888; padding: 2px;'>red (increases)</span> or <span style='background-color: #88ff88; padding: 2px;'>green (decreases)</span> based on prediction change magnitude</small>\"\n",
    "\n",
    "def on_ema_changed(change):\n",
    "    \"\"\"Handle EMA input changes.\"\"\"\n",
    "    update_graph_with_ema(change['new'])\n",
    "\n",
    "# Connect EMA input to update function\n",
    "ema_input.observe(on_ema_changed, names='value')\n",
    "\n",
    "def on_submit_clicked(b):\n",
    "    global raw_log_preds, stored_generated_tokens, stored_n_tokens\n",
    "    \n",
    "    # Reset progress\n",
    "    progress_bar.value = 0\n",
    "    percentage_label.value = \"<b>0.0%</b>\"\n",
    "    token_display.value = \"<b>Generating...</b>\"\n",
    "    \n",
    "    # Clear the graph\n",
    "    graph_widget.data = []\n",
    "    \n",
    "    # Reset global storage\n",
    "    raw_log_preds = []\n",
    "    stored_generated_tokens = []\n",
    "    stored_n_tokens = 0\n",
    "    \n",
    "    # Initialize lists to track predictions over time\n",
    "    prediction_history = []\n",
    "    token_counts = []\n",
    "    \n",
    "    # Get prompt from input\n",
    "    prompt = prompt_input.value\n",
    "    # Get EMA factor from input\n",
    "    ema_factor = ema_input.value\n",
    "    \n",
    "    # Apply chat template\n",
    "    prompt = model.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    cur_log_preds = []\n",
    "    n_tokens_generated = 0\n",
    "    generated_tokens = []\n",
    "\n",
    "    with model.generate(prompt, max_new_tokens=32768, do_sample=True) as tracer:\n",
    "        # Call .all() to apply intervention to each new token\n",
    "        with tracer.all():\n",
    "            activations = model.model.layers[15].output[0]\n",
    "            if len(activations.shape) == 1:\n",
    "                activations = activations.unsqueeze(0)\n",
    "            \n",
    "            # Save predictions within nnsight context\n",
    "            preds_saved = get_log_preds(activations, weight_tensor).save()\n",
    "            token_saved = model.lm_head.output.argmax(dim=-1).save()\n",
    "            \n",
    "            preds = preds_saved.tolist()\n",
    "            if len(preds) > 1:\n",
    "                pass\n",
    "            else:\n",
    "                cur_log_preds+=preds\n",
    "                raw_log_preds.append(preds[0])  # Store raw predictions globally\n",
    "                \n",
    "                ema_preds = get_ema_preds(torch.tensor(cur_log_preds), alpha=ema_factor)\n",
    "                n_tokens_generated+=1\n",
    "                pred_tokens_remaining = ema_preds[-1]\n",
    "                predicted_total_tokens = n_tokens_generated + pred_tokens_remaining\n",
    "                pred_percent_through = n_tokens_generated/(n_tokens_generated + pred_tokens_remaining)\n",
    "                \n",
    "                # Store prediction data for highlighting\n",
    "                prediction_history.append(predicted_total_tokens)\n",
    "                token_counts.append(n_tokens_generated)\n",
    "                \n",
    "                token = token_saved.tolist()\n",
    "                token_str = model.tokenizer.decode(token[0][0], skip_special_tokens=False)\n",
    "                # Escape HTML entities in token string for safe display\n",
    "                token_str_escaped = token_str.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;').replace('\"', '&quot;').replace(\"'\", '&#x27;')\n",
    "                print(token_str)\n",
    "                generated_tokens.append(token_str_escaped)\n",
    "                stored_generated_tokens.append(token_str_escaped)  # Store globally\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.value = pred_percent_through * 100\n",
    "                \n",
    "                # Update percentage label\n",
    "                percentage_label.value = f\"<b>{pred_percent_through*100:.1f}%</b>\"\n",
    "                \n",
    "                # Create highlighted token display with gradient colors\n",
    "                highlighted_tokens = []\n",
    "                for i, token in enumerate(generated_tokens):\n",
    "                    # Calculate percentage change if we have history\n",
    "                    if i > 0 and i < len(prediction_history):\n",
    "                        change = prediction_history[i] - prediction_history[i-1]\n",
    "                        percent_change = abs(change / prediction_history[i-1]) * 100 if prediction_history[i-1] != 0 else 0\n",
    "                        highlight_color = get_color_for_change(percent_change, change > 0)\n",
    "                    else:\n",
    "                        highlight_color = \"#e6f3ff\"  # Default light blue for first token\n",
    "                    \n",
    "                    highlighted_tokens.append(f\"<span style='background-color: {highlight_color}; padding: 2px 4px; margin: 1px; border-radius: 3px;'>{token}</span>\")\n",
    "                \n",
    "                tokens_html = \" \".join(highlighted_tokens)\n",
    "                token_display.value = f\"<b>Generated tokens:</b><br>{tokens_html}<br><br><b>Latest:</b> '{token_str_escaped}' | <b>Predicted Total:</b> {predicted_total_tokens:.0f} tokens | <b>Progress:</b> {pred_percent_through*100:.1f}%<br><br><small><b>Color coding:</b> Gradient from <span style='background-color: #e6f3ff; padding: 2px;'>neutral</span> to <span style='background-color: #ff8888; padding: 2px;'>red (increases)</span> or <span style='background-color: #88ff88; padding: 2px;'>green (decreases)</span> based on prediction change magnitude</small>\"\n",
    "                \n",
    "                # Update the graph with current predictions\n",
    "                if len(prediction_history) > 1:\n",
    "                    # Create hover text with context tokens\n",
    "                    hover_texts = []\n",
    "                    for i in range(len(generated_tokens)):\n",
    "                        # Get 5 tokens before and after (if available)\n",
    "                        start_idx = max(0, i - 5)\n",
    "                        end_idx = min(len(generated_tokens), i + 6)\n",
    "                        \n",
    "                        context_tokens = []\n",
    "                        for j in range(start_idx, end_idx):\n",
    "                            token_clean = generated_tokens[j].replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>').replace('&quot;', '\"').replace('&#x27;', \"'\")\n",
    "                            if j == i:\n",
    "                                context_tokens.append(f\"<b>{token_clean}</b>\")\n",
    "                            else:\n",
    "                                context_tokens.append(token_clean)\n",
    "                        \n",
    "                        context_str = \" \".join(context_tokens)\n",
    "                        hover_text = f\"Token {i+1}: {context_str}<br>Predicted Total: {prediction_history[i]:.0f}\"\n",
    "                        hover_texts.append(hover_text)\n",
    "                    \n",
    "                    # Update graph with new data\n",
    "                    graph_widget.data = []\n",
    "                    graph_widget.add_trace(go.Scatter(\n",
    "                        x=token_counts,\n",
    "                        y=prediction_history,\n",
    "                        mode='lines+markers',\n",
    "                        name='Predicted Total Tokens',\n",
    "                        line=dict(color='blue', width=2),\n",
    "                        marker=dict(size=6),\n",
    "                        hovertemplate='%{customdata}<extra></extra>',\n",
    "                        customdata=hover_texts\n",
    "                    ))\n",
    "                    \n",
    "                    # Add a horizontal line showing actual tokens generated so far\n",
    "                    graph_widget.add_trace(go.Scatter(\n",
    "                        x=[token_counts[0], token_counts[-1]],\n",
    "                        y=[n_tokens_generated, n_tokens_generated],\n",
    "                        mode='lines',\n",
    "                        name='Current Progress',\n",
    "                        line=dict(color='red', width=2, dash='dash'),\n",
    "                        hovertemplate='Current tokens generated: %{y}<extra></extra>'\n",
    "                    ))\n",
    "                    \n",
    "                    # Update layout\n",
    "                    graph_widget.update_layout(\n",
    "                        title='Token Prediction Over Time',\n",
    "                        xaxis_title='Token Number',\n",
    "                        yaxis_title='Predicted Total Tokens',\n",
    "                        height=400,\n",
    "                        showlegend=True,\n",
    "                        hovermode='closest'\n",
    "                    )\n",
    "    \n",
    "    # Store final token count globally\n",
    "    stored_n_tokens = n_tokens_generated\n",
    "    \n",
    "    # After generation is complete, display prediction history\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL PREDICTION HISTORY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nGenerated {n_tokens_generated} tokens total\")\n",
    "    print(f\"Final prediction was {predicted_total_tokens:.0f} tokens\")\n",
    "    print(f\"Accuracy: {(n_tokens_generated/predicted_total_tokens)*100:.1f}%\")\n",
    "    print(f\"EMA factor used: {ema_factor}\")\n",
    "    \n",
    "    print(\"\\nFull Prediction History:\")\n",
    "    print(\"Token# | Predicted Total | Change | % Change | Token\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, (count, pred_total, token) in enumerate(zip(token_counts, prediction_history, generated_tokens)):\n",
    "        if i == 0:\n",
    "            change = 0\n",
    "            percent_change = 0\n",
    "        else:\n",
    "            change = pred_total - prediction_history[i-1]\n",
    "            percent_change = abs(change / prediction_history[i-1]) * 100 if prediction_history[i-1] != 0 else 0\n",
    "        \n",
    "        # Display token in a safe way for console output (remove escaping for print)\n",
    "        token_for_print = token.replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>').replace('&quot;', '\"').replace('&#x27;', \"'\")\n",
    "        \n",
    "        # Highlight tokens with large percentage changes (>10%)\n",
    "        if percent_change > 10:\n",
    "            if change > 0:\n",
    "                # Large increase - bold green\n",
    "                token_display_str = f\"\\033[1m\\033[92m{token_for_print}\\033[0m\"\n",
    "                change_str = f\"\\033[1m\\033[92m+{change:.0f}\\033[0m\"\n",
    "            else:\n",
    "                # Large decrease - bold red  \n",
    "                token_display_str = f\"\\033[1m\\033[91m{token_for_print}\\033[0m\"\n",
    "                change_str = f\"\\033[1m\\033[91m{change:.0f}\\033[0m\"\n",
    "        else:\n",
    "            token_display_str = token_for_print\n",
    "            if change > 0:\n",
    "                change_str = f\"+{change:.0f}\"\n",
    "            else:\n",
    "                change_str = f\"{change:.0f}\"\n",
    "        \n",
    "        print(f\"{count:5d}  | {pred_total:13.0f}   | {change_str:8s} | {percent_change:6.1f}%  | {token_display_str}\")\n",
    "    \n",
    "    # Update graph with final results using current EMA\n",
    "    update_graph_with_ema(ema_factor)\n",
    "\n",
    "# Connect button click to function\n",
    "submit_button.on_click(on_submit_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea3e555b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anywidget in /usr/local/lib/python3.11/dist-packages (0.9.18)\n",
      "Requirement already satisfied: ipywidgets>=7.6.0 in /usr/local/lib/python3.11/dist-packages (from anywidget) (8.1.5)\n",
      "Requirement already satisfied: psygnal>=0.8.1 in /usr/local/lib/python3.11/dist-packages (from anywidget) (0.14.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from anywidget) (4.12.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=7.6.0->anywidget) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=7.6.0->anywidget) (9.0.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=7.6.0->anywidget) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=7.6.0->anywidget) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=7.6.0->anywidget) (3.0.13)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->anywidget) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->anywidget) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->anywidget) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->anywidget) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->anywidget) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->anywidget) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->anywidget) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->anywidget) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=7.6.0->anywidget) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=7.6.0->anywidget) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets>=7.6.0->anywidget) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from stack_data->ipython>=6.1.0->ipywidgets>=7.6.0->anywidget) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from stack_data->ipython>=6.1.0->ipywidgets>=7.6.0->anywidget) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.11/dist-packages (from stack_data->ipython>=6.1.0->ipywidgets>=7.6.0->anywidget) (0.2.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=7.6.0->anywidget) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=7.6.0->anywidget) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets>=7.6.0->anywidget) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from stack_data->ipython>=6.1.0->ipywidgets>=7.6.0->anywidget) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from stack_data->ipython>=6.1.0->ipywidgets>=7.6.0->anywidget) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.11/dist-packages (from stack_data->ipython>=6.1.0->ipywidgets>=7.6.0->anywidget) (0.2.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install anywidget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201cc79c",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Test different types of tokens to see their effects on the probe's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c11f05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_token_effects(base_text, candidate_tokens, use_chat_template=False):\n",
    "    \"\"\"\n",
    "    Analyze how appending different tokens affects the probe's prediction.\n",
    "    \n",
    "    Args:\n",
    "        base_text: The base text/context (str)\n",
    "        candidate_tokens: List of tokens/words to test appending (list of str)\n",
    "        use_chat_template: Whether to apply chat template to base_text (bool)\n",
    "    \n",
    "    Returns:\n",
    "        dict with:\n",
    "            - 'baseline_prediction': Prediction for base text alone\n",
    "            - 'token_effects': List of dicts with token, prediction, and changes\n",
    "    \"\"\"\n",
    "    # Apply chat template if requested\n",
    "    if use_chat_template:\n",
    "        base_text = model.tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": base_text}], \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    \n",
    "    # Get baseline prediction for base text\n",
    "    with model.trace(base_text):\n",
    "        activations = model.model.layers[15].output[0]\n",
    "        if len(activations.shape) == 1:\n",
    "            activations = activations.unsqueeze(0)\n",
    "        baseline_pred_tensor = get_log_preds(activations, weight_tensor).exp()[-1].save()\n",
    "    \n",
    "    baseline_pred = baseline_pred_tensor.item()\n",
    "    \n",
    "    print(f\"Baseline text: '{base_text}'\")\n",
    "    print(f\"Baseline prediction: {baseline_pred:.2f} tokens remaining\\n\")\n",
    "    \n",
    "    # Test each candidate token\n",
    "    results = []\n",
    "    for token in candidate_tokens:\n",
    "        # Append token to base text\n",
    "        test_text = base_text + token\n",
    "        \n",
    "        # Get prediction for extended text\n",
    "        with model.trace(test_text):\n",
    "            activations = model.model.layers[15].output[0]\n",
    "            if len(activations.shape) == 1:\n",
    "                activations = activations.unsqueeze(0)\n",
    "            token_pred_tensor = get_log_preds(activations, weight_tensor).exp()[-1].save()\n",
    "        \n",
    "        token_pred = token_pred_tensor.item()\n",
    "        \n",
    "        # Calculate changes\n",
    "        raw_change = token_pred - baseline_pred\n",
    "        percent_change = (raw_change / baseline_pred * 100) if baseline_pred != 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            'token': token,\n",
    "            'prediction': token_pred,\n",
    "            'raw_change': raw_change,\n",
    "            'percent_change': percent_change\n",
    "        })\n",
    "        \n",
    "        print(f\"Token: '{token:20s}' | Prediction: {token_pred:6.2f} | Change: {raw_change:+7.2f} ({percent_change:+6.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'baseline_prediction': baseline_pred,\n",
    "        'baseline_text': base_text,\n",
    "        'token_effects': results\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_token_effects(analysis_results, sort_by='raw_change'):\n",
    "    \"\"\"\n",
    "    Visualize token effects using plotly.\n",
    "    \n",
    "    Args:\n",
    "        analysis_results: Output from analyze_token_effects()\n",
    "        sort_by: 'raw_change', 'percent_change', or 'prediction'\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "    \n",
    "    baseline = analysis_results['baseline_prediction']\n",
    "    effects = analysis_results['token_effects']\n",
    "    \n",
    "    # Sort results\n",
    "    effects_sorted = sorted(effects, key=lambda x: x[sort_by], reverse=True)\n",
    "    \n",
    "    tokens = [e['token'] for e in effects_sorted]\n",
    "    predictions = [e['prediction'] for e in effects_sorted]\n",
    "    changes = [e['raw_change'] for e in effects_sorted]\n",
    "    percent_changes = [e['percent_change'] for e in effects_sorted]\n",
    "    \n",
    "    # Color based on change direction and magnitude\n",
    "    colors = []\n",
    "    for change, pct in zip(changes, percent_changes):\n",
    "        if change > 0:\n",
    "            # Red gradient for increases\n",
    "            intensity = min(abs(pct) / 50, 1.0)\n",
    "            r = int(230 + (255 - 230) * intensity)\n",
    "            g = int(243 - 243 * intensity)\n",
    "            b = int(255 - 255 * intensity)\n",
    "        else:\n",
    "            # Green gradient for decreases\n",
    "            intensity = min(abs(pct) / 50, 1.0)\n",
    "            r = int(230 - 230 * intensity)\n",
    "            g = int(243 - 39 * intensity)\n",
    "            b = int(255 - 255 * intensity)\n",
    "        colors.append(f'rgb({r},{g},{b})')\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add bars for predictions\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=tokens,\n",
    "        y=predictions,\n",
    "        marker=dict(color=colors, line=dict(color='black', width=1)),\n",
    "        text=[f\"{p:.1f}<br>({c:+.1f})\" for p, c in zip(predictions, changes)],\n",
    "        textposition='outside',\n",
    "        hovertemplate='<b>%{x}</b><br>Prediction: %{y:.2f} tokens<br>Change: %{customdata[0]:+.2f} (%{customdata[1]:+.1f}%)<extra></extra>',\n",
    "        customdata=list(zip(changes, percent_changes))\n",
    "    ))\n",
    "    \n",
    "    # Add baseline line\n",
    "    fig.add_hline(\n",
    "        y=baseline, \n",
    "        line_dash=\"dash\", \n",
    "        line_color=\"blue\",\n",
    "        annotation_text=f\"Baseline: {baseline:.2f}\",\n",
    "        annotation_position=\"right\"\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'Token Effects on Prediction<br><sub>Base: \"{analysis_results[\"baseline_text\"][:50]}...\"</sub>',\n",
    "        xaxis_title='Token',\n",
    "        yaxis_title='Predicted Tokens Remaining',\n",
    "        height=500,\n",
    "        showlegend=False,\n",
    "        hovermode='x'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Baseline prediction: {baseline:.2f} tokens\")\n",
    "    print(f\"\\nMost increase: '{effects_sorted[0]['token']}' ({effects_sorted[0]['raw_change']:+.2f}, {effects_sorted[0]['percent_change']:+.1f}%)\")\n",
    "    print(f\"Most decrease: '{effects_sorted[-1]['token']}' ({effects_sorted[-1]['raw_change']:+.2f}, {effects_sorted[-1]['percent_change']:+.1f}%)\")\n",
    "    print(f\"\\nAverage change: {sum(e['raw_change'] for e in effects) / len(effects):.2f}\")\n",
    "    print(f\"Std dev: {(sum((e['raw_change'] - sum(e['raw_change'] for e in effects) / len(effects))**2 for e in effects) / len(effects))**0.5:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6ddc410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline text: 'The weather tomorrow will be'\n",
      "Baseline prediction: 416.00 tokens remaining\n",
      "\n",
      "Token: ' alternatively      ' | Prediction: 664.00 | Change: +248.00 ( +59.6%)\n",
      "Token: ' sunny              ' | Prediction: 203.00 | Change: -213.00 ( -51.2%)\n",
      "Token: ' rainy              ' | Prediction: 296.00 | Change: -120.00 ( -28.8%)\n",
      "Token: ' probably           ' | Prediction: 1064.00 | Change: +648.00 (+155.8%)\n",
      "Token: ' maybe              ' | Prediction: 1320.00 | Change: +904.00 (+217.3%)\n",
      "Token: ' rainy              ' | Prediction: 296.00 | Change: -120.00 ( -28.8%)\n",
      "Token: ' probably           ' | Prediction: 1064.00 | Change: +648.00 (+155.8%)\n",
      "Token: ' maybe              ' | Prediction: 1320.00 | Change: +904.00 (+217.3%)\n",
      "Token: ' definitely         ' | Prediction: 776.00 | Change: +360.00 ( +86.5%)\n",
      "Token: ' perhaps            ' | Prediction: 752.00 | Change: +336.00 ( +80.8%)\n",
      "Token: ' certainly          ' | Prediction: 664.00 | Change: +248.00 ( +59.6%)\n",
      "Token: ' might              ' | Prediction: 148.00 | Change: -268.00 ( -64.4%)\n",
      "Token: ' definitely         ' | Prediction: 776.00 | Change: +360.00 ( +86.5%)\n",
      "Token: ' perhaps            ' | Prediction: 752.00 | Change: +336.00 ( +80.8%)\n",
      "Token: ' certainly          ' | Prediction: 664.00 | Change: +248.00 ( +59.6%)\n",
      "Token: ' might              ' | Prediction: 148.00 | Change: -268.00 ( -64.4%)\n",
      "Token: ' could              ' | Prediction: 880.00 | Change: +464.00 (+111.5%)\n",
      "Token: ' could              ' | Prediction: 880.00 | Change: +464.00 (+111.5%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           904,
           217.3076923076923
          ],
          [
           648,
           155.76923076923077
          ],
          [
           464,
           111.53846153846155
          ],
          [
           360,
           86.53846153846155
          ],
          [
           336,
           80.76923076923077
          ],
          [
           248,
           59.61538461538461
          ],
          [
           248,
           59.61538461538461
          ],
          [
           -120,
           -28.846153846153843
          ],
          [
           -213,
           -51.20192307692307
          ],
          [
           -268,
           -64.42307692307693
          ]
         ],
         "hovertemplate": "<b>%{x}</b><br>Prediction: %{y:.2f} tokens<br>Change: %{customdata[0]:+.2f} (%{customdata[1]:+.1f}%)<extra></extra>",
         "marker": {
          "color": [
           "rgb(255,0,0)",
           "rgb(255,0,0)",
           "rgb(255,0,0)",
           "rgb(255,0,0)",
           "rgb(255,0,0)",
           "rgb(255,0,0)",
           "rgb(255,0,0)",
           "rgb(97,220,107)",
           "rgb(0,204,0)",
           "rgb(0,204,0)"
          ],
          "line": {
           "color": "black",
           "width": 1
          }
         },
         "text": [
          "1320.0<br>(+904.0)",
          "1064.0<br>(+648.0)",
          "880.0<br>(+464.0)",
          "776.0<br>(+360.0)",
          "752.0<br>(+336.0)",
          "664.0<br>(+248.0)",
          "664.0<br>(+248.0)",
          "296.0<br>(-120.0)",
          "203.0<br>(-213.0)",
          "148.0<br>(-268.0)"
         ],
         "textposition": "outside",
         "type": "bar",
         "x": [
          " maybe",
          " probably",
          " could",
          " definitely",
          " perhaps",
          " alternatively",
          " certainly",
          " rainy",
          " sunny",
          " might"
         ],
         "y": [
          1320,
          1064,
          880,
          776,
          752,
          664,
          664,
          296,
          203,
          148
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "showarrow": false,
          "text": "Baseline: 416.00",
          "x": 1,
          "xanchor": "left",
          "xref": "x domain",
          "y": 416,
          "yanchor": "middle",
          "yref": "y"
         }
        ],
        "height": 500,
        "hovermode": "x",
        "shapes": [
         {
          "line": {
           "color": "blue",
           "dash": "dash"
          },
          "type": "line",
          "x0": 0,
          "x1": 1,
          "xref": "x domain",
          "y0": 416,
          "y1": 416,
          "yref": "y"
         }
        ],
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Token Effects on Prediction<br><sub>Base: \"The weather tomorrow will be...\"</sub>"
        },
        "xaxis": {
         "title": {
          "text": "Token"
         }
        },
        "yaxis": {
         "title": {
          "text": "Predicted Tokens Remaining"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY STATISTICS\n",
      "============================================================\n",
      "Baseline prediction: 416.00 tokens\n",
      "\n",
      "Most increase: ' maybe' (+904.00, +217.3%)\n",
      "Most decrease: ' might' (-268.00, -64.4%)\n",
      "\n",
      "Average change: 260.70\n",
      "Std dev: 356.35\n"
     ]
    }
   ],
   "source": [
    "# Example: Test uncertain vs certain words\n",
    "base = \"The weather tomorrow will be\"\n",
    "\n",
    "# Test various completion tokens\n",
    "candidates = [\n",
    "    \" alternatively\", \n",
    "    \" sunny\",      # Definite\n",
    "    \" rainy\",      # Definite\n",
    "    \" probably\",   # Uncertain\n",
    "    \" maybe\",      # Uncertain\n",
    "    \" definitely\", # Certain\n",
    "    \" perhaps\",    # Uncertain\n",
    "    \" certainly\",  # Certain\n",
    "    \" might\",      # Uncertain\n",
    "    \" could\",      # Uncertain\n",
    "]\n",
    "\n",
    "results = analyze_token_effects(base, candidates, use_chat_template=False)\n",
    "visualize_token_effects(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "621a34db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_tokens=100, use_chat_template=True):\n",
    "    \"\"\"\n",
    "    Generate text from the model.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The input prompt (str)\n",
    "        max_tokens: Maximum tokens to generate (int)\n",
    "        use_chat_template: Whether to apply chat template (bool)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (formatted_prompt, token_list, full_text)\n",
    "            - formatted_prompt: The prompt with chat template applied (if requested)\n",
    "            - token_list: List of generated token strings\n",
    "            - full_text: Complete text (formatted_prompt + generated tokens)\n",
    "    \"\"\"\n",
    "    # Apply chat template if requested\n",
    "    if use_chat_template:\n",
    "        formatted_prompt = model.tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": prompt}], \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    else:\n",
    "        formatted_prompt = prompt\n",
    "    \n",
    "    print(f\"Generating with prompt: '{prompt}'...\")\n",
    "    print(f\"Formatted prompt length: {len(formatted_prompt)} chars\\n\")\n",
    "    \n",
    "    # Use the underlying model's generate method directly\n",
    "    print(\"Generated text: \", end='')\n",
    "    \n",
    "    # Tokenize and generate using HuggingFace model\n",
    "    input_ids = model.tokenizer.encode(formatted_prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Access the wrapped model directly (nnsight wraps the HF model in ._model)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model._model.generate(\n",
    "            input_ids, \n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=model.tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Extract only generated tokens (skip prompt)\n",
    "    generated_ids = output_ids[0][input_ids.shape[1]:]\n",
    "    \n",
    "    # Decode each token individually\n",
    "    generated_tokens = []\n",
    "    for token_id in generated_ids:\n",
    "        token_str = model.tokenizer.decode([token_id.item()], skip_special_tokens=False)\n",
    "        generated_tokens.append(token_str)\n",
    "        print(token_str, end='', flush=True)\n",
    "    \n",
    "    print(f\"\\n\\nGeneration complete! Generated {len(generated_tokens)} tokens\")\n",
    "    full_text = formatted_prompt + ''.join(generated_tokens)\n",
    "    return formatted_prompt, generated_tokens, full_text\n",
    "\n",
    "def slice_tokens_to_text(formatted_prompt, token_list, slice_at):\n",
    "    \"\"\"\n",
    "    Helper function to slice token list and combine with prompt.\n",
    "    \n",
    "    Args:\n",
    "        formatted_prompt: The formatted prompt string\n",
    "        token_list: List of generated token strings\n",
    "        slice_at: Index to slice at (tokens[:slice_at])\n",
    "    \n",
    "    Returns:\n",
    "        str: formatted_prompt + sliced tokens joined\n",
    "    \"\"\"\n",
    "    return formatted_prompt + ''.join(token_list[:slice_at])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "de604cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with prompt: 'What is 2+2?'...\n",
      "Formatted prompt length: 62 chars\n",
      "\n",
      "Generated text: <think>\n",
      "Okay, the user is asking \"What is 2+2?\" That's<think>\n",
      "Okay, the user is asking \"What is 2+2?\" That's a straightforward math question. Let me start by recalling basic arithmetic. In standard addition, 2 plus 2 equals a straightforward math question. Let me start by recalling basic arithmetic. In standard addition, 2 plus 2 equals 4. But wait, maybe they're looking for something more? Like in different contexts?\n",
      "\n",
      "Hmm, maybe 4. But wait, maybe they're looking for something more? Like in different contexts?\n",
      "\n",
      "Hmm, maybe they want to know if there's any alternative interpretation. For example, in some contexts, like in they want to know if there's any alternative interpretation. For example, in some contexts, like in a different number base. If it's base 3, 2+2 would be 11 (since 2+2=4 in decimal, which is  a different number base. If it's base 3, 2+2 would be 11 (since 2+2=4 in decimal, which is 11 in base 3). But the question11 in base 3). But the question doesn't specify a base, so I should assume base 10 unless told otherwise.\n",
      "\n",
      "Alternatively, could there be a trick doesn't specify a base, so I should assume base 10 unless told otherwise.\n",
      "\n",
      "Alternatively, could there be a trick question? Sometimes people question? Sometimes people use 2+2 to refer to use 2+2 to refer to something else, like in a riddle. But I don't think so here. The user is probably just asking for the simple answer.\n",
      "\n",
      "Wait, maybe they're testing if I know the answer something else, like in a riddle. But I don't think so here. The user is probably just asking for the simple answer.\n",
      "\n",
      "Wait, maybe they're testing if I know the answer, or if there's a cultural reference. But I don't recall any specific references to 2+2 in popular, or if there's a cultural reference. But I don't recall any specific references to 2+2 in popular culture that would change the answer. So the safest bet is to state that in standard arithmetic, 2+2 equals culture that would change the answer. So the safest bet is to state that in standard arithmetic, 2+2 equals 4.\n",
      "\n",
      "Also, considering different mathematical systems, but again, unless specified, it's base 10 4.\n",
      "\n",
      "Also, considering different mathematical systems, but again, unless specified, it's base 10. So the answer should be 4. I should make sure to present it clearly and confirm that there's. So the answer should be 4. I should make sure to present it clearly and confirm that there's no other context needed\n",
      "\n",
      "Generation complete! Generated 300 tokens\n",
      "['<think>', '\\n', 'Okay', ',', ' the', ' user', ' is', ' asking', ' \"', 'What', ' is', ' ', '2', '+', '2', '?\"', ' That', \"'s\", ' a', ' straightforward', ' math', ' question', '.', ' Let', ' me', ' start', ' by', ' recalling', ' basic', ' arithmetic', '.', ' In', ' standard', ' addition', ',', ' ', '2', ' plus', ' ', '2', ' equals', ' ', '4', '.', ' But', ' wait', ',', ' maybe', ' they', \"'re\", ' looking', ' for', ' something', ' more', '?', ' Like', ' in', ' different', ' contexts', '?\\n\\n', 'Hmm', ',', ' maybe', ' they', ' want', ' to', ' know', ' if', ' there', \"'s\", ' any', ' alternative', ' interpretation', '.', ' For', ' example', ',', ' in', ' some', ' contexts', ',', ' like', ' in', ' a', ' different', ' number', ' base', '.', ' If', ' it', \"'s\", ' base', ' ', '3', ',', ' ', '2', '+', '2', ' would', ' be', ' ', '1', '1', ' (', 'since', ' ', '2', '+', '2', '=', '4', ' in', ' decimal', ',', ' which', ' is', ' ', '1', '1', ' in', ' base', ' ', '3', ').', ' But', ' the', ' question', ' doesn', \"'t\", ' specify', ' a', ' base', ',', ' so', ' I', ' should', ' assume', ' base', ' ', '1', '0', ' unless', ' told', ' otherwise', '.\\n\\n', 'Alternatively', ',', ' could', ' there', ' be', ' a', ' trick', ' question', '?', ' Sometimes', ' people', ' use', ' ', '2', '+', '2', ' to', ' refer', ' to', ' something', ' else', ',', ' like', ' in', ' a', ' r', 'iddle', '.', ' But', ' I', ' don', \"'t\", ' think', ' so', ' here', '.', ' The', ' user', ' is', ' probably', ' just', ' asking', ' for', ' the', ' simple', ' answer', '.\\n\\n', 'Wait', ',', ' maybe', ' they', \"'re\", ' testing', ' if', ' I', ' know', ' the', ' answer', ',', ' or', ' if', ' there', \"'s\", ' a', ' cultural', ' reference', '.', ' But', ' I', ' don', \"'t\", ' recall', ' any', ' specific', ' references', ' to', ' ', '2', '+', '2', ' in', ' popular', ' culture', ' that', ' would', ' change', ' the', ' answer', '.', ' So', ' the', ' safest', ' bet', ' is', ' to', ' state', ' that', ' in', ' standard', ' arithmetic', ',', ' ', '2', '+', '2', ' equals', ' ', '4', '.\\n\\n', 'Also', ',', ' considering', ' different', ' mathematical', ' systems', ',', ' but', ' again', ',', ' unless', ' specified', ',', ' it', \"'s\", ' base', ' ', '1', '0', '.', ' So', ' the', ' answer', ' should', ' be', ' ', '4', '.', ' I', ' should', ' make', ' sure', ' to', ' present', ' it', ' clearly', ' and', ' confirm', ' that', ' there', \"'s\", ' no', ' other', ' context', ' needed']\n",
      " no other context needed\n",
      "\n",
      "Generation complete! Generated 300 tokens\n",
      "['<think>', '\\n', 'Okay', ',', ' the', ' user', ' is', ' asking', ' \"', 'What', ' is', ' ', '2', '+', '2', '?\"', ' That', \"'s\", ' a', ' straightforward', ' math', ' question', '.', ' Let', ' me', ' start', ' by', ' recalling', ' basic', ' arithmetic', '.', ' In', ' standard', ' addition', ',', ' ', '2', ' plus', ' ', '2', ' equals', ' ', '4', '.', ' But', ' wait', ',', ' maybe', ' they', \"'re\", ' looking', ' for', ' something', ' more', '?', ' Like', ' in', ' different', ' contexts', '?\\n\\n', 'Hmm', ',', ' maybe', ' they', ' want', ' to', ' know', ' if', ' there', \"'s\", ' any', ' alternative', ' interpretation', '.', ' For', ' example', ',', ' in', ' some', ' contexts', ',', ' like', ' in', ' a', ' different', ' number', ' base', '.', ' If', ' it', \"'s\", ' base', ' ', '3', ',', ' ', '2', '+', '2', ' would', ' be', ' ', '1', '1', ' (', 'since', ' ', '2', '+', '2', '=', '4', ' in', ' decimal', ',', ' which', ' is', ' ', '1', '1', ' in', ' base', ' ', '3', ').', ' But', ' the', ' question', ' doesn', \"'t\", ' specify', ' a', ' base', ',', ' so', ' I', ' should', ' assume', ' base', ' ', '1', '0', ' unless', ' told', ' otherwise', '.\\n\\n', 'Alternatively', ',', ' could', ' there', ' be', ' a', ' trick', ' question', '?', ' Sometimes', ' people', ' use', ' ', '2', '+', '2', ' to', ' refer', ' to', ' something', ' else', ',', ' like', ' in', ' a', ' r', 'iddle', '.', ' But', ' I', ' don', \"'t\", ' think', ' so', ' here', '.', ' The', ' user', ' is', ' probably', ' just', ' asking', ' for', ' the', ' simple', ' answer', '.\\n\\n', 'Wait', ',', ' maybe', ' they', \"'re\", ' testing', ' if', ' I', ' know', ' the', ' answer', ',', ' or', ' if', ' there', \"'s\", ' a', ' cultural', ' reference', '.', ' But', ' I', ' don', \"'t\", ' recall', ' any', ' specific', ' references', ' to', ' ', '2', '+', '2', ' in', ' popular', ' culture', ' that', ' would', ' change', ' the', ' answer', '.', ' So', ' the', ' safest', ' bet', ' is', ' to', ' state', ' that', ' in', ' standard', ' arithmetic', ',', ' ', '2', '+', '2', ' equals', ' ', '4', '.\\n\\n', 'Also', ',', ' considering', ' different', ' mathematical', ' systems', ',', ' but', ' again', ',', ' unless', ' specified', ',', ' it', \"'s\", ' base', ' ', '1', '0', '.', ' So', ' the', ' answer', ' should', ' be', ' ', '4', '.', ' I', ' should', ' make', ' sure', ' to', ' present', ' it', ' clearly', ' and', ' confirm', ' that', ' there', \"'s\", ' no', ' other', ' context', ' needed']\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Generate and test at different slicing points\n",
    "prompt = \"What is 2+2?\"\n",
    "formatted_prompt, tokens, full_text = generate_text(prompt, max_tokens=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fd68cb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <think>\n",
      "1 \n",
      "\n",
      "2 Okay\n",
      "3 ,\n",
      "4  the\n",
      "5  user\n",
      "6  is\n",
      "7  asking\n",
      "8  \"\n",
      "9 What\n",
      "10  is\n",
      "11  \n",
      "12 2\n",
      "13 +\n",
      "14 2\n",
      "15 ?\"\n",
      "16  That\n",
      "17 's\n",
      "18  a\n",
      "19  straightforward\n",
      "20  math\n",
      "21  question\n",
      "22 .\n",
      "23  Let\n",
      "24  me\n",
      "25  start\n",
      "26  by\n",
      "27  recalling\n",
      "28  basic\n",
      "29  arithmetic\n",
      "30 .\n",
      "31  In\n",
      "32  standard\n",
      "33  addition\n",
      "34 ,\n",
      "35  \n",
      "36 2\n",
      "37  plus\n",
      "38  \n",
      "39 2\n",
      "40  equals\n",
      "41  \n",
      "42 4\n",
      "43 .\n",
      "44  But\n",
      "45  wait\n",
      "46 ,\n",
      "47  maybe\n",
      "48  they\n",
      "49 're\n",
      "50  looking\n",
      "51  for\n",
      "52  something\n",
      "53  more\n",
      "54 ?\n",
      "55  Like\n",
      "56  in\n",
      "57  different\n",
      "58  contexts\n",
      "59 ?\n",
      "\n",
      "\n",
      "60 Hmm\n",
      "61 ,\n",
      "62  maybe\n",
      "63  they\n",
      "64  want\n",
      "65  to\n",
      "66  know\n",
      "67  if\n",
      "68  there\n",
      "69 's\n",
      "70  any\n",
      "71  alternative\n",
      "72  interpretation\n",
      "73 .\n",
      "74  For\n",
      "75  example\n",
      "76 ,\n",
      "77  in\n",
      "78  some\n",
      "79  contexts\n",
      "80 ,\n",
      "81  like\n",
      "82  in\n",
      "83  a\n",
      "84  different\n",
      "85  number\n",
      "86  base\n",
      "87 .\n",
      "88  If\n",
      "89  it\n",
      "90 's\n",
      "91  base\n",
      "92  \n",
      "93 3\n",
      "94 ,\n",
      "95  \n",
      "96 2\n",
      "97 +\n",
      "98 2\n",
      "99  would\n",
      "100  be\n",
      "101  \n",
      "102 1\n",
      "103 1\n",
      "104  (\n",
      "105 since\n",
      "106  \n",
      "107 2\n",
      "108 +\n",
      "109 2\n",
      "110 =\n",
      "111 4\n",
      "112  in\n",
      "113  decimal\n",
      "114 ,\n",
      "115  which\n",
      "116  is\n",
      "117  \n",
      "118 1\n",
      "119 1\n",
      "120  in\n",
      "121  base\n",
      "122  \n",
      "123 3\n",
      "124 ).\n",
      "125  But\n",
      "126  the\n",
      "127  question\n",
      "128  doesn\n",
      "129 't\n",
      "130  specify\n",
      "131  a\n",
      "132  base\n",
      "133 ,\n",
      "134  so\n",
      "135  I\n",
      "136  should\n",
      "137  assume\n",
      "138  base\n",
      "139  \n",
      "140 1\n",
      "141 0\n",
      "142  unless\n",
      "143  told\n",
      "144  otherwise\n",
      "145 .\n",
      "\n",
      "\n",
      "146 Alternatively\n",
      "147 ,\n",
      "148  could\n",
      "149  there\n",
      "150  be\n",
      "151  a\n",
      "152  trick\n",
      "153  question\n",
      "154 ?\n",
      "155  Sometimes\n",
      "156  people\n",
      "157  use\n",
      "158  \n",
      "159 2\n",
      "160 +\n",
      "161 2\n",
      "162  to\n",
      "163  refer\n",
      "164  to\n",
      "165  something\n",
      "166  else\n",
      "167 ,\n",
      "168  like\n",
      "169  in\n",
      "170  a\n",
      "171  r\n",
      "172 iddle\n",
      "173 .\n",
      "174  But\n",
      "175  I\n",
      "176  don\n",
      "177 't\n",
      "178  think\n",
      "179  so\n",
      "180  here\n",
      "181 .\n",
      "182  The\n",
      "183  user\n",
      "184  is\n",
      "185  probably\n",
      "186  just\n",
      "187  asking\n",
      "188  for\n",
      "189  the\n",
      "190  simple\n",
      "191  answer\n",
      "192 .\n",
      "\n",
      "\n",
      "193 Wait\n",
      "194 ,\n",
      "195  maybe\n",
      "196  they\n",
      "197 're\n",
      "198  testing\n",
      "199  if\n",
      "200  I\n",
      "201  know\n",
      "202  the\n",
      "203  answer\n",
      "204 ,\n",
      "205  or\n",
      "206  if\n",
      "207  there\n",
      "208 's\n",
      "209  a\n",
      "210  cultural\n",
      "211  reference\n",
      "212 .\n",
      "213  But\n",
      "214  I\n",
      "215  don\n",
      "216 't\n",
      "217  recall\n",
      "218  any\n",
      "219  specific\n",
      "220  references\n",
      "221  to\n",
      "222  \n",
      "223 2\n",
      "224 +\n",
      "225 2\n",
      "226  in\n",
      "227  popular\n",
      "228  culture\n",
      "229  that\n",
      "230  would\n",
      "231  change\n",
      "232  the\n",
      "233  answer\n",
      "234 .\n",
      "235  So\n",
      "236  the\n",
      "237  safest\n",
      "238  bet\n",
      "239  is\n",
      "240  to\n",
      "241  state\n",
      "242  that\n",
      "243  in\n",
      "244  standard\n",
      "245  arithmetic\n",
      "246 ,\n",
      "247  \n",
      "248 2\n",
      "249 +\n",
      "250 2\n",
      "251  equals\n",
      "252  \n",
      "253 4\n",
      "254 .\n",
      "\n",
      "\n",
      "255 Also\n",
      "256 ,\n",
      "257  considering\n",
      "258  different\n",
      "259  mathematical\n",
      "260  systems\n",
      "261 ,\n",
      "262  but\n",
      "263  again\n",
      "264 ,\n",
      "265  unless\n",
      "266  specified\n",
      "267 ,\n",
      "268  it\n",
      "269 's\n",
      "270  base\n",
      "271  \n",
      "272 1\n",
      "273 0\n",
      "274 .\n",
      "275  So\n",
      "276  the\n",
      "277  answer\n",
      "278  should\n",
      "279  be\n",
      "280  \n",
      "281 4\n",
      "282 .\n",
      "283  I\n",
      "284  should\n",
      "285  make\n",
      "286  sure\n",
      "287  to\n",
      "288  present\n",
      "289  it\n",
      "290  clearly\n",
      "291  and\n",
      "292  confirm\n",
      "293  that\n",
      "294  there\n",
      "295 's\n",
      "296  no\n",
      "297  other\n",
      "298  context\n",
      "299  needed\n"
     ]
    }
   ],
   "source": [
    "for index, n in enumerate(tokens):\n",
    "    print(index, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ef31ec71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user is asking \"What is 2+2?\" That\\'s a straightforward math question. Let me start by recalling basic arithmetic. In standard addition, 2 plus 2 equals '"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = ''.join(tokens[:42])\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb4f6195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1011988\n",
      "1010988\n",
      "1009988\n",
      "1008988\n",
      "1007988\n",
      "1006988\n",
      "1005988\n",
      "1004988\n",
      "1003988\n",
      "1002988\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10000, 1000):\n",
    "    print(1011988 - i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58abcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what the chat template produces\n",
    "user_message = \"Tell me a story\"\n",
    "formatted = model.tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": user_message}], \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(\"Full formatted prompt:\")\n",
    "print(repr(formatted))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Visual:\")\n",
    "print(formatted)\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find the assistant suffix (what comes after the user message)\n",
    "user_end_idx = formatted.find(user_message) + len(user_message)\n",
    "assistant_suffix = formatted[user_end_idx:]\n",
    "print(f\"\\nAssistant suffix (after user message): {repr(assistant_suffix)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80a64250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nhello world<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"hello world\"}], \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b139178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_template(prompt):\n",
    "    return f'<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d769e099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline text: '<|im_start|>user\n",
      "Which is the better university, UCL or KCL?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "Baseline prediction: 444.00 tokens remaining\n",
      "\n",
      "Token: 'UCL                 ' | Prediction: 1064.00 | Change: +620.00 (+139.6%)\n",
      "Token: 'KCL                 ' | Prediction: 1280.00 | Change: +836.00 (+188.3%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           836,
           188.2882882882883
          ],
          [
           620,
           139.63963963963963
          ]
         ],
         "hovertemplate": "<b>%{x}</b><br>Prediction: %{y:.2f} tokens<br>Change: %{customdata[0]:+.2f} (%{customdata[1]:+.1f}%)<extra></extra>",
         "marker": {
          "color": [
           "rgb(255,0,0)",
           "rgb(255,0,0)"
          ],
          "line": {
           "color": "black",
           "width": 1
          }
         },
         "text": [
          "1280.0<br>(+836.0)",
          "1064.0<br>(+620.0)"
         ],
         "textposition": "outside",
         "type": "bar",
         "x": [
          "KCL",
          "UCL"
         ],
         "y": [
          1280,
          1064
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "showarrow": false,
          "text": "Baseline: 444.00",
          "x": 1,
          "xanchor": "left",
          "xref": "x domain",
          "y": 444,
          "yanchor": "middle",
          "yref": "y"
         }
        ],
        "height": 500,
        "hovermode": "x",
        "shapes": [
         {
          "line": {
           "color": "blue",
           "dash": "dash"
          },
          "type": "line",
          "x0": 0,
          "x1": 1,
          "xref": "x domain",
          "y0": 444,
          "y1": 444,
          "yref": "y"
         }
        ],
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Token Effects on Prediction<br><sub>Base: \"<|im_start|>user\nWhich is the better university, U...\"</sub>"
        },
        "xaxis": {
         "title": {
          "text": "Token"
         }
        },
        "yaxis": {
         "title": {
          "text": "Predicted Tokens Remaining"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY STATISTICS\n",
      "============================================================\n",
      "Baseline prediction: 444.00 tokens\n",
      "\n",
      "Most increase: 'KCL' (+836.00, +188.3%)\n",
      "Most decrease: 'UCL' (+620.00, +139.6%)\n",
      "\n",
      "Average change: 728.00\n",
      "Std dev: 108.00\n"
     ]
    }
   ],
   "source": [
    "base = insert_template(\"Which is the better university, UCL or KCL?\")\n",
    "\n",
    "# Test how different next words affect the prediction at this point\n",
    "test_words = [\n",
    "    \"UCL\",\n",
    "    \"KCL\"\n",
    "]\n",
    "\n",
    "results = analyze_token_effects(base, test_words, use_chat_template=False)\n",
    "visualize_token_effects(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d77c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Test at multiple slice points from the same generation\n",
    "prompt = \"The solution to this problem\"\n",
    "formatted_prompt, tokens, full_text = generate_text(prompt, max_tokens=40)\n",
    "\n",
    "# Test at different points in the generation\n",
    "test_points = [5, 10, 15, 20]\n",
    "test_continuations = [\" is\", \" might\", \" could\", \" definitely\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING AT MULTIPLE POINTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for point in test_points:\n",
    "    if point < len(tokens):\n",
    "        base = slice_tokens_to_text(formatted_prompt, tokens, point)\n",
    "        print(f\"\\n\\nPoint {point}: '{base[-50:]}...'\")  # Show last 50 chars\n",
    "        \n",
    "        results = analyze_token_effects(base, test_continuations, use_chat_template=False)\n",
    "        print(f\"Most impactful: {max(results['token_effects'], key=lambda x: abs(x['raw_change']))['token']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Super flexible workflow - generate once, analyze anywhere\n",
    "prompt = \"I think the answer\"\n",
    "formatted_prompt, tokens, full_text = generate_text(prompt, max_tokens=25)\n",
    "\n",
    "# Now you have tokens as a list - slice however you want!\n",
    "print(f\"\\n\\nYou can now use Python slicing:\")\n",
    "print(f\"tokens[:5]   -> first 5 tokens\")\n",
    "print(f\"tokens[5:10] -> tokens 5-10\") \n",
    "print(f\"tokens[-5:]  -> last 5 tokens\")\n",
    "print(f\"tokens[::2]  -> every other token\")\n",
    "\n",
    "# Pick any slice point you want\n",
    "my_slice = 8  # Change this!\n",
    "base = slice_tokens_to_text(formatted_prompt, tokens, my_slice)\n",
    "\n",
    "print(f\"\\n\\nAnalyzing at token {my_slice}:\")\n",
    "print(f\"Base text: '{base}'\")\n",
    "\n",
    "# Your test tokens\n",
    "my_tests = [\" is\", \" was\", \" might\", \" should\", \" could\", \" will\"]\n",
    "results = analyze_token_effects(base, my_tests, use_chat_template=False)\n",
    "visualize_token_effects(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
